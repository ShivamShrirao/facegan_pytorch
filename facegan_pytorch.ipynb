{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "facegan_pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bFvHCGVA-Q6r",
        "outputId": "633fd072-e520-4288-a15e-7ac660a46202"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat Jul 10 19:36:03 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.42.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   45C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbF0D9-gAIUX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71d05755-bdeb-4307-a11a-ec33117748ea"
      },
      "source": [
        "%pip install -q --extra-index-url https://developer.download.nvidia.com/compute/redist --upgrade nvidia-dali-cuda110\n",
        "%pip install -q wandb"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 621.4MB 26kB/s \n",
            "\u001b[K     |████████████████████████████████| 1.8MB 39.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 102kB 14.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 143kB 50.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 174kB 46.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 11.1MB/s \n",
            "\u001b[?25h  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIeQ0xHq-aqL",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "752dfa9a-1e54-4ddb-beff-96acb7c60543"
      },
      "source": [
        "# from google.colab import files\n",
        "# ret = files.upload()\n",
        "!mkdir  ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d jessicali9530/celeba-dataset"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-3a9308a6-4ef5-46a9-b178-955fc5017d14\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-3a9308a6-4ef5-46a9-b178-955fc5017d14\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "Downloading celeba-dataset.zip to /content\n",
            "100% 1.33G/1.33G [00:06<00:00, 225MB/s]\n",
            "100% 1.33G/1.33G [00:06<00:00, 219MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckdwv-e_-evY"
      },
      "source": [
        "!unzip -q celeba-dataset.zip\n",
        "!rm celeba-dataset.zip"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dU-VM_CG9Km"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLkIHo4R-gip"
      },
      "source": [
        "%load_ext tensorboard\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJ-IGTrx-ova"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import autograd\n",
        "from torch.cuda import amp\n",
        "\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import PIL.Image as Image\n",
        "from collections import OrderedDict\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['figure.facecolor'] = 'white'"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_athS9wr4Lu"
      },
      "source": [
        "import wandb"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGM0uvd9r42K"
      },
      "source": [
        "config_defaults = {\n",
        "    'BATCH_SIZE'        : 128,\n",
        "    'noise_dim'         : 128,\n",
        "    'input_size'        : 128,\n",
        "    'dp_rate'           : 0.3,\n",
        "    'gauss_std'         : 0.1,\n",
        "    'std_decay_rate'    : 0.,#1/50,\n",
        "    'guassian_noise'    : ['discriminator'], #, 'generator']\n",
        "    'nthreads'          : 2 * len(os.sched_getaffinity(0)),\n",
        "    'max_lr'            : 2e-4,\n",
        "    'betas'             : (0.5, 0.999),\n",
        "    'seed'              : 614,\n",
        "    'use_amp'           : True,\n",
        "    'log_interval'      : 10,\n",
        "    \"z_notes\"           : ['lossD/2']\n",
        "}\n",
        "CONFIG = config_defaults"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruN404CCr5PM"
      },
      "source": [
        "# #hide\n",
        "# run = wandb.init(id='19sqz0by', project=\"facegan_pytorch\", resume='must')\n",
        "# CONFIG = run.config"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "id": "R83YKK2hr84h",
        "outputId": "dddf776c-b923-43ce-8696-4ce5c86cc36d"
      },
      "source": [
        "run = wandb.init(project=\"facegan_pytorch\", config=config_defaults)\n",
        "CONFIG = wandb.config"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "wandb: Paste an API key from your profile and hit enter: ··········\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.33<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">legendary-glade-21</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/shivamshrirao/facegan_pytorch\" target=\"_blank\">https://wandb.ai/shivamshrirao/facegan_pytorch</a><br/>\n",
              "                Run page: <a href=\"https://wandb.ai/shivamshrirao/facegan_pytorch/runs/3groprms\" target=\"_blank\">https://wandb.ai/shivamshrirao/facegan_pytorch/runs/3groprms</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210710_194017-3groprms</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIxwyb-3-5Hs"
      },
      "source": [
        "def seed_everything(seed=33):\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    # torch.backends.cudnn.deterministic = True\n",
        "\n",
        "seed_everything()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ljk8xCMBDIOp"
      },
      "source": [
        "class AverageMeter:\n",
        "    def __init__(self, name=None):\n",
        "        self.name = name\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = self.sum = self.count = self.avg = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNMFa5ke_85-"
      },
      "source": [
        "from torchvision import transforms as T\n",
        "\n",
        "from nvidia.dali.pipeline import Pipeline\n",
        "import nvidia.dali.fn as fn\n",
        "import nvidia.dali.types as types\n",
        "from nvidia.dali.plugin.pytorch import DALIClassificationIterator, LastBatchPolicy"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xI_oNKS4_6yv"
      },
      "source": [
        "def get_preproc(input_size):\n",
        "    return T.Compose([\n",
        "        T.RandomResizedCrop(input_size, interpolation=T.InterpolationMode.BICUBIC),\n",
        "        T.RandomHorizontalFlip(input_size),\n",
        "        lambda image: image.convert(\"RGB\"),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
        "    ])\n",
        "\n",
        "\n",
        "def dali_pipeline(batch_size, num_threads, device_id, image_dir, input_size):\n",
        "    pipe = Pipeline(batch_size, num_threads, device_id)\n",
        "    with pipe:\n",
        "        jpegs, lbls = fn.readers.file(file_root=image_dir, random_shuffle=True, name=\"Reader\")\n",
        "        images = fn.decoders.image(jpegs, device='mixed', output_type=types.RGB)\n",
        "        images = fn.resize(images, device='gpu', resize_shorter=input_size, interp_type=types.INTERP_TRIANGULAR)\n",
        "        mirror = fn.random.coin_flip(probability=0.5)\n",
        "        images = fn.crop_mirror_normalize(images,\n",
        "                                          dtype=types.FLOAT,\n",
        "                                          output_layout=\"CHW\",\n",
        "                                          crop=(input_size, input_size),\n",
        "                                          mean=[0.5 * 255, 0.5 * 255, 0.5 * 255],\n",
        "                                          std=[0.5 * 255, 0.5 * 255, 0.5 * 255],\n",
        "                                          mirror=mirror)\n",
        "        pipe.set_outputs(images, lbls)\n",
        "    return pipe"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9j3JDXBazj6e",
        "outputId": "85b18214-45c1-4d38-e498-9e3647a61023"
      },
      "source": [
        "CONFIG['nthreads']"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "je-jAyqkAVr8"
      },
      "source": [
        "pipe = dali_pipeline(batch_size=CONFIG['BATCH_SIZE'], num_threads=CONFIG[\"nthreads\"], device_id=0, image_dir=\"/content/img_align_celeba/\", input_size=CONFIG[\"input_size\"])\n",
        "train_loader = DALIClassificationIterator(pipe, auto_reset=True, reader_name=\"Reader\", last_batch_policy=LastBatchPolicy.PARTIAL)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8RpgGza_RZY"
      },
      "source": [
        "class GaussianNoise(nn.Module):\n",
        "    def __init__(self, std=0.1, decay_rate=0):\n",
        "        super().__init__()\n",
        "        self.std = std\n",
        "        self.decay_rate = decay_rate\n",
        "\n",
        "    def decay_step(self):\n",
        "        self.std = max(self.std - self.decay_rate, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + torch.empty_like(x).normal_(std=self.std)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLQY0yib_SEf"
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, noise_dim, activation=nn.ReLU, dp_rate=0.3):\n",
        "        super().__init__()\n",
        "        self.activation = activation\n",
        "        self.stem = nn.Sequential(OrderedDict([\n",
        "            ('linear',  nn.Linear(noise_dim, 512*4*4, bias=False)),\n",
        "            ('bn',      nn.LazyBatchNorm1d()),\n",
        "            ('act',     activation(inplace=True)),\n",
        "            ('dropout', nn.Dropout(dp_rate)),       # try 2d for spatial\n",
        "        ]))\n",
        "\n",
        "        self.stacks = nn.Sequential(*[\n",
        "            self.upsample(512, dp_rate=dp_rate),\n",
        "            self.upsample(256, dp_rate=dp_rate),\n",
        "            self.upsample(128, dp_rate=dp_rate),\n",
        "            self.upsample(64, dp_rate=0)\n",
        "        ])\n",
        "\n",
        "        self.gen = nn.Sequential(OrderedDict([\n",
        "            ('conv',    nn.LazyConvTranspose2d(3, kernel_size=4, stride=2, padding=1)),\n",
        "            ('act',     nn.Tanh()),\n",
        "        ]))\n",
        "\n",
        "    def upsample(self, num_filters, bn=True, dp_rate=0.3):\n",
        "        layers = [nn.LazyConvTranspose2d(num_filters, kernel_size=4, stride=2, bias=not bn, padding=1)]\n",
        "        if bn:\n",
        "            layers.append(nn.BatchNorm2d(num_filters))\n",
        "        layers.append(self.activation(inplace=True))\n",
        "        if dp_rate > 0:\n",
        "            layers.append(nn.Dropout(dp_rate))      # try 2d for spatial\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.stem(x)\n",
        "        x = x.view(-1, 512, 4, 4)\n",
        "        x = self.stacks(x)\n",
        "        x = self.gen(x)\n",
        "        return x"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTJPshbl_VF6"
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, activation=nn.LeakyReLU, std=0.1, std_decay_rate=0):\n",
        "        super().__init__()\n",
        "        self.std = std\n",
        "        self.std_decay_rate = std_decay_rate\n",
        "        self.activation = activation\n",
        "        self.stacks = nn.Sequential(*[\n",
        "            self.downsample(32, bn=False, stride=1),\n",
        "            self.downsample(64),\n",
        "            self.downsample(128),\n",
        "            self.downsample(256),\n",
        "            self.downsample(512)\n",
        "        ])\n",
        "\n",
        "        self.head = nn.Sequential(OrderedDict([\n",
        "            ('gauss', GaussianNoise(self.std, self.std_decay_rate)),\n",
        "            ('linear', nn.LazyLinear(1)),\n",
        "            # ('act', nn.Sigmoid()),        # removed for BCEWithLogitsLoss\n",
        "        ]))\n",
        "\n",
        "    def downsample(self, num_filters, bn=True, stride=2):\n",
        "        layers = [\n",
        "            GaussianNoise(self.std, self.std_decay_rate),\n",
        "            nn.LazyConv2d(num_filters, kernel_size=4, stride=stride, bias=not bn, padding=1)\n",
        "        ]\n",
        "        if bn:\n",
        "            layers.append(nn.BatchNorm2d(num_filters))\n",
        "        layers.append(self.activation(0.2, inplace=True))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.stacks(x)\n",
        "        x = x.flatten(1)\n",
        "        x = self.head(x)\n",
        "        return x"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEvizYK2_Amq"
      },
      "source": [
        "@torch.no_grad()\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VoVawbzC_Xcb"
      },
      "source": [
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbuK7xPs_ci8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d97add8b-8a45-4344-9ca6-8975d04385eb"
      },
      "source": [
        "netG = Generator(CONFIG['noise_dim'], dp_rate=CONFIG['dp_rate']).to(DEVICE)\n",
        "netD = Discriminator(std=CONFIG['gauss_std'], std_decay_rate=CONFIG['std_decay_rate']).to(DEVICE)\n",
        "\n",
        "netG.apply(weights_init)\n",
        "netD.apply(weights_init)\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
            "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjgGrKRb_h3z"
      },
      "source": [
        "optG = torch.optim.Adam(netG.parameters(), lr=CONFIG['max_lr'], betas=CONFIG['betas'])\n",
        "optD = torch.optim.Adam(netD.parameters(), lr=CONFIG['max_lr'], betas=CONFIG['betas'])\n",
        "\n",
        "scaler = amp.GradScaler(enabled=CONFIG['use_amp'])"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmvbFnD0_Xzh"
      },
      "source": [
        "# def train_epoch(train_loader, netG, netD, optG, optD, scaler, noise_dim, epoch=1, use_amp=False, log_interval=10):\n",
        "#     netG.train()\n",
        "#     netD.train()\n",
        "#     lossesG = AverageMeter()\n",
        "#     lossesD = AverageMeter()\n",
        "#     optG.zero_grad(set_to_none=True)\n",
        "#     optD.zero_grad(set_to_none=True)\n",
        "#     with tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch}\") as pbar:\n",
        "#         for idx, batch in pbar:\n",
        "#             real_images = batch[0]['data']\n",
        "#             noise = torch.randn(real_images.size(0), noise_dim, dtype=real_images.dtype,\n",
        "#                                 device=real_images.device)\n",
        "#             # Update generator\n",
        "#             with amp.autocast(enabled=use_amp):\n",
        "#                 fake_out = netD(netG(noise))\n",
        "#                 lossG = criterion(fake_out, torch.ones_like(fake_out))  # Treat fake images as real to train the Generator.\n",
        "\n",
        "#             netG.requires_grad_(True)           # Only calculate gradients for Generator.\n",
        "#             netD.requires_grad_(False)          # Do not calculate gradients for Discriminator.\n",
        "#             scaler.scale(lossG).backward(retain_graph=True) # retain graph cause fake_out is also used to calculate loss for Discriminator.\n",
        "#             # Update the generator later.\n",
        "            \n",
        "#             lossesG.update(lossG.detach_(), noise.size(0))\n",
        "\n",
        "#             # Update Discriminator\n",
        "#             with amp.autocast(enabled=use_amp):\n",
        "#                 real_out = netD(real_images)\n",
        "#                 lossD = (criterion(real_out, torch.empty_like(real_out).uniform_(0.9, 1.0))\n",
        "#                         + criterion(fake_out, torch.empty_like(fake_out).uniform_(0.0, 0.1)))   # Treat real as real and fake as fake to train Discriminator.\n",
        "\n",
        "#             netG.requires_grad_(False)          # Do not calculate gradients for Generator.\n",
        "#             netD.requires_grad_(True)           # Only calculate gradients for Discriminator.\n",
        "#             scaler.scale(lossD).backward(retain_graph=True)\n",
        "#             scaler.step(optD)\n",
        "#             optD.zero_grad(set_to_none=True)\n",
        "\n",
        "#             scaler.step(optG)\n",
        "#             optG.zero_grad(set_to_none=True)\n",
        "\n",
        "#             scaler.update()\n",
        "\n",
        "#             lossesD.update(lossD.detach_(), real_images.size(0))\n",
        "#             if not idx%log_interval:\n",
        "#                 info = {'Generator_loss': float(lossesG.avg), 'Discriminator_loss': float(lossesD.avg)}\n",
        "#                 wandb.log(info)\n",
        "#                 pbar.set_postfix(info)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFNXRm2mkkKC"
      },
      "source": [
        "def set_grads(grads, params):\n",
        "    for g,p in zip(grads, params):\n",
        "        p.grad = g\n",
        "\n",
        "def train_epoch(train_loader, netG, netD, optG, optD, scaler, noise_dim, epoch=1, use_amp=True, log_interval=10):\n",
        "    netG.train()\n",
        "    netD.train()\n",
        "    lossesG = AverageMeter()\n",
        "    lossesD = AverageMeter()\n",
        "    optG.zero_grad(set_to_none=True)\n",
        "    optD.zero_grad(set_to_none=True)\n",
        "    with tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch:>2}\") as pbar:\n",
        "        for idx, batch in pbar:\n",
        "            real_images = batch[0]['data']\n",
        "            noise = torch.randn(real_images.size(0), noise_dim, dtype=real_images.dtype,\n",
        "                                device=real_images.device)\n",
        "            with amp.autocast(enabled=use_amp):\n",
        "                fake_out = netD(netG(noise))\n",
        "                real_out = netD(real_images)\n",
        "                lossG = criterion(fake_out, torch.ones_like(fake_out))                          # Treat fake images as real to train the Generator.\n",
        "                lossD = (criterion(real_out, torch.empty_like(real_out).uniform_(0.9, 1.0))     # Treat real as real\n",
        "                        + criterion(fake_out, torch.empty_like(fake_out).uniform_(0.0, 0.1)))   # and fake as fake to train Discriminator.\n",
        "\n",
        "            scaled_gradsG = autograd.grad(scaler.scale(lossG), netG.parameters(), retain_graph=True)\n",
        "            scaled_gradsD = autograd.grad(scaler.scale(lossD), netD.parameters())\n",
        "\n",
        "            set_grads(scaled_gradsG, netG.parameters())\n",
        "            set_grads(scaled_gradsD, netD.parameters())\n",
        "\n",
        "            scaler.step(optD)\n",
        "            optD.zero_grad(set_to_none=True)\n",
        "            scaler.step(optG)\n",
        "            optG.zero_grad(set_to_none=True)\n",
        "            scaler.update()\n",
        "\n",
        "            lossesG.update(lossG.detach_(), noise.size(0))\n",
        "            lossesD.update(lossD.detach_(), real_images.size(0))\n",
        "            if not idx%log_interval:\n",
        "                info = {'Generator_loss': float(lossesG.avg), 'Discriminator_loss': float(lossesD.avg)}\n",
        "                wandb.log(info)\n",
        "                pbar.set_postfix(info)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1I1o5j43nYo"
      },
      "source": [
        "@torch.no_grad()\n",
        "def gen_grid(ncols=4, nrows=4, figsize=[16,16], show=True, wb_log=True):\n",
        "    netG.eval()\n",
        "    noise = torch.randn(nrows*ncols, CONFIG['noise_dim'], dtype=torch.float32, device=DEVICE)\n",
        "    gen = (netG(noise).detach_()+1)/2\n",
        "    gen = gen.cpu().permute(0,2,3,1).numpy()       # convert to channel last\n",
        "    fig, ax = plt.subplots(nrows=nrows, ncols=ncols, figsize=figsize, gridspec_kw = {'wspace':0, 'hspace':0})\n",
        "    fig.patch.set_facecolor('white')\n",
        "    ims = []\n",
        "    for i,axi in enumerate(ax.flat):\n",
        "        axi.axis(\"off\")\n",
        "        axi.imshow(gen[i])\n",
        "        if wb_log: ims.append(wandb.Image(gen[i]))\n",
        "    if wb_log: wandb.log({\"Generated\": ims})\n",
        "    if show:\n",
        "        plt.show()\n",
        "    else:\n",
        "        plt.close(fig)\n",
        "    # return gen"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MzLeJBR-HO_"
      },
      "source": [
        "def decay_gauss_std(net):\n",
        "    for m in net.modules():\n",
        "        if isinstance(m, GaussianNoise):\n",
        "            m.decay_step()"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-a9W4aNE3Nl"
      },
      "source": [
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hg5FUdDNE2CT"
      },
      "source": [
        "init_epoch = 1\n",
        "NUM_EPOCHS = 50"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHNHYmqjavTK",
        "outputId": "715639e8-72e8-458e-93b0-e84c4509eed4"
      },
      "source": [
        "#collapse-output\n",
        "for epoch in range(init_epoch, NUM_EPOCHS+1):\n",
        "    train_loss = train_epoch(train_loader,\n",
        "                             netG = netG,\n",
        "                             netD = netD,\n",
        "                             optG = optG,\n",
        "                             optD = optD,\n",
        "                             scaler = scaler,\n",
        "                             noise_dim = CONFIG['noise_dim'],\n",
        "                             epoch=epoch,\n",
        "                             use_amp=CONFIG['use_amp'],\n",
        "                             log_interval=CONFIG['log_interval'])\n",
        "    \n",
        "    gen_grid(ncols=4, nrows=2, figsize=[16, 8], show=False)\n",
        "    gc.collect()\n",
        "    decay_gauss_std(netD)\n",
        "    # if not epoch%10:\n",
        "    #     save_model(model, optimizer, epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch  1: 100%|██████████| 1583/1583 [07:13<00:00,  3.65it/s, Generator_loss=2.83, Discriminator_loss=0.355]\n",
            "Epoch  2: 100%|██████████| 1583/1583 [07:08<00:00,  3.70it/s, Generator_loss=2.48, Discriminator_loss=0.356]\n",
            "Epoch  3: 100%|██████████| 1583/1583 [07:08<00:00,  3.70it/s, Generator_loss=2.51, Discriminator_loss=0.347]\n",
            "Epoch  4: 100%|██████████| 1583/1583 [07:07<00:00,  3.70it/s, Generator_loss=2.46, Discriminator_loss=0.347]\n",
            "Epoch  5: 100%|██████████| 1583/1583 [07:08<00:00,  3.70it/s, Generator_loss=2.4, Discriminator_loss=0.364]\n",
            "Epoch  6: 100%|██████████| 1583/1583 [07:07<00:00,  3.70it/s, Generator_loss=2.39, Discriminator_loss=0.354]\n",
            "Epoch  7: 100%|██████████| 1583/1583 [07:06<00:00,  3.71it/s, Generator_loss=2.45, Discriminator_loss=0.347]\n",
            "Epoch  8: 100%|██████████| 1583/1583 [07:07<00:00,  3.71it/s, Generator_loss=2.48, Discriminator_loss=0.36]\n",
            "Epoch  9: 100%|██████████| 1583/1583 [07:07<00:00,  3.70it/s, Generator_loss=2.48, Discriminator_loss=0.34]\n",
            "Epoch 10: 100%|██████████| 1583/1583 [07:06<00:00,  3.71it/s, Generator_loss=2.59, Discriminator_loss=0.324]\n",
            "Epoch 11: 100%|██████████| 1583/1583 [07:06<00:00,  3.71it/s, Generator_loss=2.67, Discriminator_loss=0.321]\n",
            "Epoch 12: 100%|██████████| 1583/1583 [07:06<00:00,  3.71it/s, Generator_loss=2.68, Discriminator_loss=0.323]\n",
            "Epoch 13: 100%|██████████| 1583/1583 [07:06<00:00,  3.71it/s, Generator_loss=2.74, Discriminator_loss=0.306]\n",
            "Epoch 14: 100%|██████████| 1583/1583 [07:06<00:00,  3.71it/s, Generator_loss=2.8, Discriminator_loss=0.306]\n",
            "Epoch 15: 100%|██████████| 1583/1583 [07:06<00:00,  3.71it/s, Generator_loss=2.75, Discriminator_loss=0.319]\n",
            "Epoch 16: 100%|██████████| 1583/1583 [07:06<00:00,  3.71it/s, Generator_loss=2.85, Discriminator_loss=0.298]\n",
            "Epoch 17: 100%|██████████| 1583/1583 [07:06<00:00,  3.71it/s, Generator_loss=2.82, Discriminator_loss=0.285]\n",
            "Epoch 18: 100%|██████████| 1583/1583 [07:06<00:00,  3.71it/s, Generator_loss=2.89, Discriminator_loss=0.282]\n",
            "Epoch 19: 100%|██████████| 1583/1583 [07:06<00:00,  3.71it/s, Generator_loss=2.9, Discriminator_loss=0.294]\n",
            "Epoch 20: 100%|██████████| 1583/1583 [07:06<00:00,  3.71it/s, Generator_loss=2.9, Discriminator_loss=0.283]\n",
            "Epoch 21: 100%|██████████| 1583/1583 [07:06<00:00,  3.71it/s, Generator_loss=2.87, Discriminator_loss=0.303]\n",
            "Epoch 22: 100%|██████████| 1583/1583 [07:06<00:00,  3.71it/s, Generator_loss=2.95, Discriminator_loss=0.259]\n",
            "Epoch 23: 100%|██████████| 1583/1583 [07:06<00:00,  3.71it/s, Generator_loss=2.9, Discriminator_loss=0.294]\n",
            "Epoch 24: 100%|██████████| 1583/1583 [07:06<00:00,  3.72it/s, Generator_loss=2.98, Discriminator_loss=0.276]\n",
            "Epoch 25: 100%|██████████| 1583/1583 [07:06<00:00,  3.71it/s, Generator_loss=2.91, Discriminator_loss=0.269]\n",
            "Epoch 26: 100%|██████████| 1583/1583 [07:06<00:00,  3.71it/s, Generator_loss=2.97, Discriminator_loss=0.27]\n",
            "Epoch 27: 100%|██████████| 1583/1583 [07:06<00:00,  3.71it/s, Generator_loss=2.95, Discriminator_loss=0.273]\n",
            "Epoch 28: 100%|██████████| 1583/1583 [07:06<00:00,  3.71it/s, Generator_loss=3.02, Discriminator_loss=0.254]\n",
            "Epoch 29: 100%|██████████| 1583/1583 [07:06<00:00,  3.71it/s, Generator_loss=2.97, Discriminator_loss=0.27]\n",
            "Epoch 30: 100%|██████████| 1583/1583 [07:06<00:00,  3.71it/s, Generator_loss=3, Discriminator_loss=0.259]\n",
            "Epoch 31:  81%|████████  | 1276/1583 [05:43<01:21,  3.76it/s, Generator_loss=2.99, Discriminator_loss=0.274]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zY9tw0_XazXr"
      },
      "source": [
        "_ = gen_grid(wb_log=False)  # newer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "riXWwDCxZPD8"
      },
      "source": [
        "run.finish()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}